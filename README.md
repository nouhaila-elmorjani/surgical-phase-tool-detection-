# ğŸ©º Surgical Phase & Tool Recognition

A **multitask deep learning framework** for laparoscopic surgery video analysis, jointly recognizing **surgical phases** and **surgical tools** from short temporal windows of frames.
The system integrates **clinical workflow constraints** through a **phase â†’ tool hierarchy**, improving robustness, interpretability, and practical usability.

---

## âœ¨ Highlights

* Joint **surgical phase recognition** and **tool detection**
* Clinically informed **phase-conditioned tool predictions**
* Temporal windowâ€“based video modeling
* Fully reproducible training and evaluation pipeline
* Command-line inference and interactive visualization
* Centralized YAML-based configuration

Designed for **research, benchmarking, and experimental deployment** in surgical workflow understanding.

---

## ğŸ§  Conceptual Overview

Laparoscopic procedures follow a well-defined sequence of surgical phases, and only a subset of instruments is clinically plausible within each phase.
This project exploits that structure by:

1. Predicting the **current surgical phase**
2. Using the predicted phase to **constrain tool predictions**
3. Producing **clinically consistent and interpretable outputs**

This design reduces implausible predictions and improves tool detection reliability.

---

## ğŸ—ï¸ Repository Structure

```
.
â”œâ”€â”€ app.py                     # Streamlit interface for interactive inspection
â”œâ”€â”€ train.py                   # Training pipeline
â”œâ”€â”€ evaluate.py                # Test evaluation and metric export
â”œâ”€â”€ demo.py                    # CLI inference on frame folders
â”œâ”€â”€ config_loader.py           # YAML loader, seed and device utilities
â”œâ”€â”€ config.py                  # Backward-compatible config access
â”œâ”€â”€ dataset.py                 # Temporal multitask dataset
â”œâ”€â”€ hierarchy/
â”‚   â””â”€â”€ phase_tool_mask.py     # Phase â†’ tool validity definitions
â”œâ”€â”€ metrics.py                 # Phase and tool evaluation metrics
â”œâ”€â”€ models/
â”‚   â””â”€â”€ resnet_multitask.py    # Multitask ResNet architecture
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ report_utils.py        # Reporting helpers
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ final_config.yaml      # Central configuration file
â”œâ”€â”€ notebooks/                 # Analysis and visualization notebooks
â””â”€â”€ requirements.txt           # Dependencies
```

---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/nouhaila-elmorjani/surgical-phase-tool-detection
cd surgical-phase-tool-detection
```

### 2. Create a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate      # macOS / Linux
# .venv\Scripts\activate       # Windows
```

### 3. Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ—‚ï¸ Dataset & Configuration

All paths, hyperparameters, and runtime options are defined in:

```
configs/final_config.yaml
```

### Key configuration fields

* **Data**

  * `data_root`: Root directory of the processed dataset
  * `paths.processed_root`: Directory containing extracted frames
  * `paths.splits_multi_task`: Train/validation/test splits
  * `paths.train_manifest`, `paths.test_manifest`: CSV manifests

* **Model**

  * `model.num_phases`
  * `model.num_tools`

* **Training**

  * `training.window_size`
  * `training.image_size`
  * `training.batch_size`
  * `training.learning_rate`
  * `training.seed`

* **Runtime**

  * `runtime.device`: `"cpu"`, `"cuda"`, or `"auto"`

* **Checkpoint**

  * `checkpoint.path`: Location of the saved model

âš ï¸ Update `data_root` before running training or evaluation.

---

## ğŸš€ Training

Train the model from scratch:

```bash
python train.py
```

The training pipeline:

* Sets global random seeds
* Builds temporal window datasets
* Computes class weights to address phase imbalance
* Optimizes a weighted multitask objective:

  * Phase classification loss
  * Tool detection loss with hierarchy masking
* Logs metrics to:

  ```
  logs/training_log.csv
  ```
* Saves the best checkpoint based on **validation phase accuracy**

---

## ğŸ“Š Evaluation

Evaluate a trained model on the test set:

```bash
python evaluate.py
```

This script computes:

* Overall and per-class phase accuracy
* Phase confusion matrix
* Tool precision, recall, and F1 scores
* Tool metrics **with and without hierarchy masking**

Results are written to:

```
logs/eval_summary.csv
logs/eval_tools.csv
```

---

## ğŸ§ª CLI Inference Demo

Run inference on a directory of frames:

```bash
python demo.py \
  --input_frames path/to/frames_folder \
  --checkpoint path/to/checkpoint.pth
```

---

## ğŸ–¥ï¸ Interactive Streamlit App

Launch the interactive interface:

```bash
streamlit run app.py
```

The app allows you to:

* Upload one or more frames
* Inspect phase predictions and probability distributions
* Compare raw vs hierarchy-masked tool predictions
* Visualize phase evolution across frames
* Inspect the active phase â†’ tool validity mask

---

## ğŸ§© Model & Hierarchy Design

### Architecture

* Shared **ResNet** backbone
* Two task-specific heads:

  * **Phase head** (softmax)
  * **Tool head** (sigmoid)

### Phase â†’ Tool Hierarchy

Clinical constraints are encoded as a **binary validity matrix** defining which tools are plausible in each surgical phase.

The hierarchy is applied:

* **During training**: invalid tools are excluded from the loss
* **During inference**: tool probabilities are filtered based on the predicted phase

---

## ğŸ” Reproducibility

Reproducibility is supported through:

* Fixed random seeds (Python, NumPy, PyTorch)
* Centralized YAML configuration

---

## ğŸ‘¤ Author

**ELMORJANI Nouhaila**
GitHub: [https://github.com/nouhaila-elmorjani](https://github.com/nouhaila-elmorjani)

---

## ğŸ™ Acknowledgments

This work builds upon and is inspired by prior research and open resources in surgical workflow analysis, including:

* **Surgformer: Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition**
  Yang, Shu; Luo, Luyang; Wang, Qiong; Chen, Hao
  MICCAI 2024 (Open Access)
  [https://papers.miccai.org/miccai-2024/paper/1220_paper.pdf](https://papers.miccai.org/miccai-2024/paper/1220_paper.pdf)
  [https://github.com/isyangshu/Surgformer](https://github.com/isyangshu/Surgformer)

* **PhaKIR Dataset â€“ Surgical Phase, Keypoint, and Instrument Recognition**
  Tobias Rueckert *et al.*
  MICCAI 2024 / EndoVis Challenge
  A multi-institutional dataset providing frame-level annotations for surgical phase and instrument recognition, enabling research on temporally consistent surgical scene understanding.

We thank the authors and organizers for making these resources available to the research community.

---

